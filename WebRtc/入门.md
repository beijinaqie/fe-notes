采样频率： 采样设备（麦克风）每秒进行的采样次数（每台电脑的麦克风都有固定的采样频率，可以通过 AudioContext 对象进行查看，这个对象后续介绍。
采样位数： 简单理解为每次采样所占的位数。后续要加深理解
采样声道： 一般分为单声道和双声道 可以理解为在声音采集时从一个点进行收集，双声道从两个点进行收集

# AudioContext
AudioContext() 构造方法创建了一个新的 AudioContext 对象 它代表了一个由音频模块链接而成的音频处理图，每一个模块由 AudioNode 表示。
```js
var audioCtx = new (window.AudioContext || window.webkitAudioContext)();

// audioWorklet: AudioWorklet {}
// baseLatency: 0.005333333333333333
// currentTime: 1.5946666666666667
// destination: AudioDestinationNode {
  // maxChannelCount: 2, 
  // context: AudioContext, 
  // numberOfInputs: 1, 
  // numberOfOutputs: 0, 
  // channelCount: 2,
  // }
// listener: AudioListener {positionX: AudioParam, positionY: AudioParam, positionZ: AudioParam, forwardX: AudioParam, forwardY: AudioParam, …}
// onstatechange: null
// sampleRate: 48000
// state: "running"


```
## createBufferSource
createBufferSource() 方法用于创建一个新的AudioBufferSourceNode接口，该接口可以通过AudioBuffer 对象来播放音频数据。AudioBuffer对象可以通过AudioContext.createBuffer 来创建或者通过 AudioContext.decodeAudioData成功解码音轨后获取。
```js
var source = audioCtx.createBufferSource();
```


```js
var audioCtx = new (window.AudioContext || window.webkitAudioContext)();
var button = document.querySelector('button');
var pre = document.querySelector('pre');
var myScript = document.querySelector('script');

pre.innerHTML = myScript.innerHTML;

// Stereo
var channels = 2;
// Create an empty two second stereo buffer at the
// sample rate of the AudioContext
var frameCount = audioCtx.sampleRate * 2.0;

var myArrayBuffer = audioCtx.createBuffer(2, frameCount, audioCtx.sampleRate);

button.onclick = function() {
  // Fill the buffer with white noise;
  //just random values between -1.0 and 1.0
  for (var channel = 0; channel < channels; channel++) {
   // This gives us the actual ArrayBuffer that contains the data
   var nowBuffering = myArrayBuffer.getChannelData(channel);
   for (var i = 0; i < frameCount; i++) {
     // Math.random() is in [0; 1.0]
     // audio needs to be in [-1.0; 1.0]
     nowBuffering[i] = Math.random() * 2 - 1;
   }
  }

  // Get an AudioBufferSourceNode.
  // This is the AudioNode to use when we want to play an AudioBuffer
  var source = audioCtx.createBufferSource();
  // set the buffer in the AudioBufferSourceNode
  source.buffer = myArrayBuffer;
  // connect the AudioBufferSourceNode to the
  // destination so we can hear the sound
  source.connect(audioCtx.destination);
  // start the source playing
  source.start();
}

```

## createMediaStreamDestination
AudioContext接口的 createMediaStreamDestination() 方法用于创建一个新的对象，该对象关联着表示音频流的一个 WebRTC (en-US) MediaStream，音频流可以存储在本地文件或者被发送到另外一台计算机。

## destination
AudioContext的 destination 属性返回一个 AudioDestinationNode，表示 context 中所有音频的最终目标节点，一般是音频渲染设备，比如扬声器。